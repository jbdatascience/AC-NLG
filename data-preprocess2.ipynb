{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/admin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/admin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/admin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/admin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/admin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing file ./data/datav4/finished_files/train.bin\n",
      "\n",
      "Writing vocab file...\n",
      "Finished writing vocab file\n",
      "Finished writing file ./data/datav4/finished_files/val.bin\n",
      "\n",
      "Splitting train data into chunks...\n",
      "./data/datav4/finished_files/train.bin\n",
      "Splitting val data into chunks...\n",
      "./data/datav4/finished_files/val.bin\n",
      "Saved chunked data in ./data/datav4/finished_files/chunked\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import collections\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "TRAIN_FILE = \"./data/train_art_summ_prep_os.txt\"\n",
    "VAL_FILE = \"./data/val_art_summ_prep_os.txt\"\n",
    "\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "VOCAB_SIZE = 50000  \n",
    "CHUNK_SIZE = 1000  \n",
    "\n",
    "FINISHED_FILE_DIR = './data/datav4/finished_files'\n",
    "CHUNKS_DIR = os.path.join(FINISHED_FILE_DIR, 'chunked')\n",
    "\n",
    "\n",
    "def chunk_file(finished_files_dir, chunks_dir, name, chunk_size):\n",
    "    in_file = os.path.join(finished_files_dir, '%s.bin' % name)\n",
    "    print(in_file)\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (name, chunk)) \n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(chunk_size):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1\n",
    "\n",
    "\n",
    "def chunk_all():\n",
    "    if not os.path.isdir(CHUNKS_DIR):\n",
    "        os.mkdir(CHUNKS_DIR)\n",
    "    for name in ['train', 'val']:\n",
    "        print(\"Splitting %s data into chunks...\" % name)\n",
    "        chunk_file(FINISHED_FILE_DIR, CHUNKS_DIR, name, CHUNK_SIZE)\n",
    "    print(\"Saved chunked data in %s\" % CHUNKS_DIR)\n",
    "\n",
    "\n",
    "def read_text_file(text_file):\n",
    "    lines = []\n",
    "    with open(text_file, \"r\", encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip())\n",
    "    return lines\n",
    "\n",
    "\n",
    "def write_to_bin(input_file, out_file, makevocab=False):\n",
    "    if makevocab:\n",
    "        vocab_counter = collections.Counter()\n",
    "\n",
    "    with open(out_file, 'wb') as writer:\n",
    "        lines = read_text_file(input_file)\n",
    "        for i, new_line in enumerate(lines):\n",
    "            if i % 4 == 0:\n",
    "                results = lines[i]\n",
    "            if i % 4 == 1:\n",
    "                requests = lines[i]\n",
    "            if i % 4 == 2:\n",
    "                article = lines[i]\n",
    "            if i % 4 == 3:\n",
    "                abstract = \"%s %s %s\" % (SENTENCE_START, lines[i], SENTENCE_END)\n",
    "\n",
    "                # 写入tf.Example\n",
    "                tf_example = example_pb2.Example()\n",
    "                tf_example.features.feature['results'].bytes_list.value.extend([bytes(results, encoding='utf-8')])\n",
    "                tf_example.features.feature['requests'].bytes_list.value.extend([bytes(requests, encoding='utf-8')])\n",
    "                tf_example.features.feature['article'].bytes_list.value.extend([bytes(article, encoding='utf-8')])\n",
    "                tf_example.features.feature['abstract'].bytes_list.value.extend([bytes(abstract, encoding='utf-8')])\n",
    "                tf_example_str = tf_example.SerializeToString()\n",
    "                str_len = len(tf_example_str)\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "\n",
    "                if makevocab:\n",
    "                    rst_tokens = results.split(' ')\n",
    "                    rqs_tokens = requests.split(' ')\n",
    "                    art_tokens = article.split(' ')\n",
    "                    abs_tokens = abstract.split(' ')\n",
    "                    abs_tokens = [t for t in abs_tokens if\n",
    "                                  t not in [SENTENCE_START, SENTENCE_END]]  \n",
    "                    tokens = rst_tokens + rqs_tokens + art_tokens + abs_tokens\n",
    "                    tokens = [t.strip() for t in tokens]  \n",
    "                    tokens = [t for t in tokens if t != \"\"]\n",
    "                    vocab_counter.update(tokens)\n",
    "\n",
    "    print(\"Finished writing file %s\\n\" % out_file)\n",
    "\n",
    "    if makevocab:\n",
    "        print(\"Writing vocab file...\")\n",
    "        with open(os.path.join(FINISHED_FILE_DIR, \"vocab\"), 'w', encoding='utf-8') as writer:\n",
    "            for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "                writer.write(word + ' ' + str(count) + '\\n')\n",
    "        print(\"Finished writing vocab file\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(FINISHED_FILE_DIR):\n",
    "        os.makedirs(FINISHED_FILE_DIR)\n",
    "    write_to_bin(TRAIN_FILE, os.path.join(FINISHED_FILE_DIR, \"train.bin\"), makevocab=True)\n",
    "    write_to_bin(VAL_FILE, os.path.join(FINISHED_FILE_DIR, \"val.bin\"))\n",
    "\n",
    "    chunk_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
